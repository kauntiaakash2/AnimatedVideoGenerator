{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtSYxVnRIhhl7Fb1LPXvY+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kauntiaakash2/AnimatedVideoGenerator/blob/main/AnimatedVideoGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YfIA1psL-FqM"
      },
      "outputs": [],
      "source": [
        "%pip install google-genai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "b17-FePf-omF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# This line will fail if the secret 'secretName' is not configured\n",
        "# in the Colab notebook's secrets panel, or if notebook access is not enabled.\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "kEkxtfMr_zvn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq && apt-get install -qq locales\n",
        "!locale-gen en_US.UTF-8\n",
        "!update-locale LANG=en US.UTF-8 LC_ALL=en_US.UTF-8\n",
        "\n",
        "!apt-get -qq -y install espeak-ng > /dev/null 2>&1\n",
        "%pip install -q google-generativeai moviepy Pillow\n",
        "%pip install -q nest_asyncio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qljrEnyw__sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Core Data Processing\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "\n",
        "#Image Handling\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "#For Video And Audio\n",
        "from moviepy.editor import ImageClip, AudioFileClip, CompositeVideoClip, concatenate_videoclips\\\n",
        "\n",
        "import time\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "import typing_extensions as typing\n",
        "\n",
        "#async support\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import asyncio\n",
        "import contextlib\n",
        "import wave\n",
        "\n",
        "#Google GenAI\n",
        "from google import genai"
      ],
      "metadata": {
        "id": "_92xXtyPB_Th"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "client = genai.Client(http_options = {\n",
        "    'api_version':'v1alpha'\n",
        "})\n",
        "\n",
        "MODEL = \"models/gemini-2.0-flash-exp\"\n",
        "\n",
        "IMAGE_MODEL_ID = \"imagen-3.0-generate-002\""
      ],
      "metadata": {
        "id": "xT4rqtU0Ek7j"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StorySegment(typing.TypedDict):\n",
        "  image_prompt: str\n",
        "  audio_text: str\n",
        "  character_description:str\n",
        "\n",
        "\n",
        "class StoryResponse(typing.TypedDict):\n",
        "  complete_story: list[StorySegment]\n",
        "  pages:int\n",
        "\n",
        "def generate_story_sequence(complete_story:str, pages:int) -> list[StoryResponse]:\n",
        "  response = client.models.generate_content(\n",
        "      model=MODEL,\n",
        "      contents=f'''you are an animation video producer. Generate a story sequence about {complete_story} in {pages} scenes (with interactions and characters), 1 sec each scene. Write:\n",
        "\n",
        "image_prompt:(define art style for kids animation(consistent for all the characters), no violence) a full description of the scene, the characters in it, and the background in 20 words or less. Progressively shift the scene as the story advances.\n",
        "audio_text: a one-sentence dialogue/narration for the scene.\n",
        "character_description: no people ever, only animals and objects. Describe all characters (consistent names, features, clothing, etc.) with an art style reference (e.g., \"Pixar style,\" \"photorealistic,\" \"Ghibli\") in 30 words or less.\n",
        "''',\n",
        "      config={\n",
        "          'response_mime_type':'application/json',\n",
        "          'response_schema': list[StoryResponse]\n",
        "      }\n",
        "  )\n",
        "  try:\n",
        "    story_data_text = response.text\n",
        "    story_data_list = json.loads(story_data_text)\n",
        "    if isinstance(story_data_list,list) and len(story_data_list)>0:\n",
        "      story_data = story_data_list[0]\n",
        "      return story_data.get('complete_story',[]),story_data.get('character_description',{})\n",
        "    else:\n",
        "      return[]\n",
        "  except (KeyError, TypeError, IndexError, json.JSONDecodeError) as e:\n",
        "    print(f\"Error parsing story data: {e}\")\n",
        "    return[]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f9D8xNuOF3JW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theme = \"Monkey Steal's Two Cat Food Using Weighting Machine\"\n",
        "num_scenes = 3\n",
        "\n",
        "story_segments, _ = generate_story_sequence(theme, num_scenes)\n",
        "print(json.dumps(story_segments,indent=2))"
      ],
      "metadata": {
        "id": "pco09HmPJLQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@contextlib.contextmanager\n",
        "def wave_file(filename, channels=1, rate=24000, sample_width=2):\n",
        "    with wave.open(filename, \"wb\") as wf:\n",
        "        wf.setnchannels(channels)\n",
        "        wf.setsampwidth(sample_width)\n",
        "        wf.setframerate(rate)\n",
        "        yield wf"
      ],
      "metadata": {
        "id": "WpnDdoVuKMG3"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 2: Definitions and setup ---\n",
        "temp_audio_files = []  # To track temporary audio files\n",
        "temp_image_files = []  # To track temporary image files\n",
        "video_clips = []       # To store individual video clips for each scene\n",
        "\n",
        "def generate_audio_live(api_text, output_filename):\n",
        "    import asyncio\n",
        "    collected_audio = bytearray()\n",
        "\n",
        "    async def _generate():\n",
        "        config = {\n",
        "            \"response_modalities\": [\"AUDIO\"]\n",
        "        }\n",
        "        # Connect to the Live API using the client already initialized above.\n",
        "        async with client.aio.live.connect(model=MODEL, config=config) as session:\n",
        "            # Send the audio_text prompt; mark as end_of_turn.\n",
        "            await session.send(input=api_text, end_of_turn=True)\n",
        "            # Collect audio data as it streams in.\n",
        "            async for response in session.receive():\n",
        "                if response.data:\n",
        "                    collected_audio.extend(response.data)\n",
        "        return bytes(collected_audio)\n",
        "\n",
        "    # Run the async function and collect the audio bytes.\n",
        "    audio_bytes = asyncio.run(_generate())\n",
        "    # Write the collected audio bytes into a WAV file using the helper.\n",
        "    with wave_file(output_filename) as wf:\n",
        "        wf.writeframes(audio_bytes)\n",
        "    return output_filename\n",
        "\n",
        "\n",
        "\n",
        "# Note: Use a system instruction to prevent common AI responses and ensure natural narration\n",
        "audio_negative_prompt = \"don't say OK , I will do this or that, just only read this story using voice expressions without introductions or ending ,more segments are coming ,don't say OK , I will do this or that:\\n\""
      ],
      "metadata": {
        "id": "r0BJ7k0QKU0r"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 3: Main processing loop ---\n",
        "for i, segment in enumerate(story_segments):\n",
        "    # Retrieve details for the current scene.\n",
        "    image_prompt = segment['image_prompt']\n",
        "    audio_text =  audio_negative_prompt + segment['audio_text']\n",
        "    audio_text_prompt = segment['audio_text']\n",
        "    char_desc = segment['character_description']\n",
        "    print(f\"Processing scene {i}:\")\n",
        "    print(\"Image Prompt:\", image_prompt)\n",
        "    print(\"Audio Text:\", audio_text_prompt)\n",
        "    print(\"Character Description:\", char_desc)\n",
        "    print(\"--------------------------------\")\n",
        "\n",
        "    # -------------------------\n",
        "    # Image Generation using Google Imagen\n",
        "    # -------------------------\n",
        "    combined_prompt = \"detailed children book animation style \" + image_prompt + \" \" + char_desc\n",
        "\n",
        "    result = client.models.generate_images(\n",
        "        model=IMAGE_MODEL_ID,\n",
        "        prompt=combined_prompt,\n",
        "        config={\n",
        "            \"number_of_images\": 1,\n",
        "            \"output_mime_type\": \"image/jpeg\",\n",
        "            \"person_generation\": \"DONT_ALLOW\",\n",
        "            \"aspect_ratio\": \"1:1\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        if not result.generated_images:\n",
        "            raise ValueError(\"No images were generated. The prompt might have been flagged as harmful. Please modify your prompt and try again.\")\n",
        "        for generated_image in result.generated_images:\n",
        "            image = Image.open(BytesIO(generated_image.image.image_bytes))\n",
        "    except Exception as e:\n",
        "        print(\"Image generation failed \", e)\n",
        "\n",
        "    image_path = f\"image_{i}.png\"\n",
        "    image.save(image_path)\n",
        "    temp_image_files.append(image_path)\n",
        "    display(image)\n",
        "\n",
        "    # -------------------------\n",
        "    # Audio Generation using Google Live API\n",
        "    # -------------------------\n",
        "    audio_path = f\"audio_{i}.wav\"\n",
        "    audio_path = generate_audio_live(audio_text, audio_path)\n",
        "    temp_audio_files.append(audio_path)\n",
        "\n",
        "\n",
        "    # -------------------------\n",
        "    # Create Video Clip (Image + Audio)\n",
        "    # -------------------------\n",
        "    audio_clip = AudioFileClip(audio_path)\n",
        "\n",
        "    # Convert PIL Image to numpy array\n",
        "    np_image = np.array(image)\n",
        "\n",
        "    # Create ImageClip (size is inferred from np_image)\n",
        "    image_clip = ImageClip(np_image).set_duration(audio_clip.duration)\n",
        "\n",
        "    # Store composite clip with audio in memory\n",
        "    composite_clip = CompositeVideoClip([image_clip]).set_audio(audio_clip)\n",
        "    video_clips.append(composite_clip)"
      ],
      "metadata": {
        "id": "WQE4y0-VKZJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_video = concatenate_videoclips(video_clips)\n",
        "output_filename = f\"{int(time.time())}_output_video.mp4\"\n",
        "print(\"Writing final video to\", output_filename)\n",
        "final_video.write_videofile(output_filename, fps=24)\n",
        "\n",
        "# Display the video in the notebook\n",
        "def show_video(video_path):\n",
        "    \"\"\"Display video in notebook\"\"\"\n",
        "    video_file = open(video_path, \"rb\")\n",
        "    video_bytes = video_file.read()\n",
        "    video_b64 = b64encode(video_bytes).decode()\n",
        "    video_tag = f'<video width=\"640\" height=\"480\" controls><source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\"></video>'\n",
        "    return HTML(video_tag)\n",
        "\n",
        "# Show the video\n",
        "display(show_video(output_filename))\n",
        "\n",
        "# Cleanup: Close video clips and remove temporary files\n",
        "final_video.close()\n",
        "for clip in video_clips:\n",
        "    clip.close()\n",
        "for file in temp_audio_files:\n",
        "    os.remove(file)\n",
        "for file in temp_image_files:\n",
        "    os.remove(file)\n",
        "\n",
        "\n",
        "# A video player will appear below"
      ],
      "metadata": {
        "id": "vlB7IsorMfkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jgt4bDZKMs-4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}